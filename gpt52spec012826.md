# techspec.md — FDA 510(k) Review Studio v2.1 “Regulatory Command Center” (WOW UI + Dataset Studio)

**Deployment Target:** Hugging Face Spaces (Streamlit, single-container)  
**Core Principle:** Human-in-the-loop regulatory review acceleration through (1) document ingestion + OCR, (2) cross-database context lookup, (3) configurable multi-agent orchestration, and (4) analyst-grade dataset ingestion/standardization via **Dataset Studio**—all presented in a WOW split-pane “Regulatory Command Center” UI.

---

## 1. Executive Summary

The **FDA 510(k) Review Studio v2.1** is a Streamlit-based agentic workspace designed to speed up and standardize medical device regulatory review workflows. It combines:

1. **Document Processing**: PDF upload, page trimming, text extraction, OCR (local and Vision).
2. **Context Engine**: Fast fuzzy search across four datasets (510(k), Recalls, MDR/ADR, GUDID) and a consolidated **360° Device View**.
3. **Agent Orchestration (Human-in-the-loop)**: Configurable agents defined in `agents.yaml`, governed by global rules in `SKILL.md`. Users can edit prompts, models, and tokens before each run and edit the output that feeds into the next agent.
4. **AI Note Keeper**: Turn pasted/uploaded notes into organized Markdown with Coral-highlighted keywords and additional AI “Magics”.
5. **Dataset Studio (NEW in v2.1)**: Paste or upload datasets (CSV/JSON/TXT) for any of the four dataset types, automatically standardize them into canonical schemas, preview the first 20 records, download standardized outputs (CSV/JSON), generate a comprehensive dataset summary (1000–2000 words) using user-selected LLMs, and run dataset Q&A prompts over filtered or full dataset context. Results are editable in Markdown and render with Coral highlighting.

The experience is packaged as a “WOW UI” with:
- Light/Dark themes
- English/Traditional Chinese (繁體中文)
- 20 painter-inspired style palettes + Jackpot randomizer
- Status indicators for key readiness signals (API keys, dataset counts, OCR readiness)
- A top dashboard with KPIs and “Review Mana” gamification

v2.1 addresses a Streamlit state management pitfall by ensuring widget keys do **not** collide with session state keys (preventing `StreamlitAPIException` when assigning to `st.session_state[...]` for a widget-managed key).

---

## 2. Product Goals and Non-Goals

### 2.1 Goals
- Enable reviewers to rapidly transform unstructured submissions into structured review artifacts (e.g., gap lists, traceability matrices, deficiency drafts).
- Provide a cohesive UI that supports “read → extract → search → analyze → compile” flows without losing state across Streamlit reruns.
- Support multi-provider LLM orchestration with safe API key handling.
- Provide dataset ingestion and standardization so reviewers can work with:
  - provided defaults, or
  - user-supplied datasets that may not be pre-standardized.
- Provide transparent, auditable outputs: show what data was used and avoid fabricated conclusions.

### 2.2 Non-Goals (Current Release)
- Full RAG/vector database search at scale (Chroma/FAISS) for million-row datasets.
- Persistent storage across sessions (HF Spaces typically ephemeral; this app is session-state based).
- Full PDF annotation layer with click-to-jump evidence references (planned enhancement).

---

## 3. System Architecture

### 3.1 Deployment Model
- Single Streamlit app: `app.py` (monolithic file)  
- External configuration files:
  - `agents.yaml` (agent definitions)
  - `SKILL.md` (global rules appended into agent system prompts)
  - `requirements.txt` (Python dependencies)
- Optional OS packages via HF `packages.txt` (recommended):
  - `poppler-utils` for `pdf2image`
  - `tesseract-ocr` for local OCR

### 3.2 Layered Design (Conceptual)
**Frontend/UI Layer**
- Theme / language / painter style selectors (Top Bar popover)
- Split-pane layout for Command Center
- Tabs for Note Keeper and Dataset Studio
- Dashboard KPIs and status chips

**Logic Layer**
- PDF trimming/extraction/OCR pipeline
- Search engine (cross-dataset fuzzy search + 360 view aggregation)
- Agent runner (multi-provider LLM calls)
- Dataset Studio parsing + standardization + summarization + querying

**Data Layer**
- In-memory Pandas DataFrames for:
  - 510(k)
  - MDR/ADR
  - GUDID
  - Recall
- User may replace any dataset at runtime through Dataset Studio; the search engine is rebuilt against session datasets.

### 3.3 State Management
All user work persists across reruns using `st.session_state`, including:
- Current theme/language/style
- Uploaded PDFs and trimmed bytes
- Extracted text / OCR text
- Agents and SKILL content edits
- Agent output history and edits for chaining
- Note Keeper content and renders
- Dataset Studio loaded DataFrames, standardization report, summary and query outputs

**Key Constraint (Bugfix Awareness):**  
Streamlit will throw an exception if a widget is created with `key="X"` and the code also assigns to `st.session_state["X"]` in the same run. v2.1 avoids collisions by using distinct widget keys like `ds_input_text_widget` while storing the canonical value in `st.session_state["ds_input_text"]`.

---

## 4. UI/UX Specification — “WOW Regulatory Command Center”

### 4.1 Visual Theme: Painter’s Studio + Glassmorphism
- Background: obsidian dark (`#0B1020`) or clean light (`#F6F7FB`), with subtle radial gradients.
- Glass cards: translucent panels with blur and borders to create a “command center” feel.
- Accent: selected painter style accent color affects decorative UI elements.
- Coral (`#FF7F50`): reserved for critical regulatory signals and highlighted keywords.

### 4.2 Global Controls
- Theme: light/dark
- Language: EN / 繁體中文
- Painter Style: 20 options + Jackpot random selection

### 4.3 Top Bar Status Indicators
- API key readiness chips (OpenAI, Gemini, Anthropic, xAI):
  - Green: present in environment, “Managed by System”
  - Yellow: provided in-session
  - Red: missing
- Dataset counters: number of rows loaded for each dataset type
- OCR status: ready/empty
- “Review Mana”: progress bar derived from agent run count for subtle gamification

### 4.4 Mode Control
- **Command Center**: main workflow (PDF + search + agents + final report + dataset studio tab)
- **AI Note Keeper**: focused note ingestion and transformation flow

---

## 5. Default Datasets and Canonical Schemas

### 5.1 Default Datasets
The app ships with embedded default datasets (small sample) for:
- 510(k)
- MDR/ADR
- GUDID
- Recall

These are loaded into session DataFrames at startup and displayed in status counters.

### 5.2 Canonical Schema Definitions (Dataset Studio Standardization Targets)
Dataset Studio standardizes user-supplied data into these canonical field lists:

**510(k):**
- `k_number`, `decision_date`, `decision`, `device_name`, `applicant`, `manufacturer_name`,
  `product_code`, `regulation_number`, `device_class`, `panel`, `review_advisory_committee`,
  `predicate_k_numbers` (list), `summary`

**ADR/MDR:**
- `adverse_event_id`, `report_date`, `event_type`, `patient_outcome`, `device_problem`,
  `manufacturer_name`, `brand_name`, `product_code`, `device_class`, `udi_di`,
  `recall_number_link`, `narrative`

**GUDID:**
- `primary_di`, `udi_di`, `device_description`, `device_class`, `manufacturer_name`, `brand_name`,
  `product_code`, `gmdn_term`, `mri_safety`, `sterile`, `single_use`, `implantable`, `contains_nrl`,
  `version_or_model_number`, `catalog_number`, `record_status`, `publish_date`,
  `company_contact_email`, `company_contact_phone`, `company_state`, `company_country`

**Recall:**
- `recall_number`, `recall_class`, `event_date`, `termination_date`, `status`, `firm_name`,
  `manufacturer_name`, `product_description`, `product_code`, `code_info`, `reason_for_recall`,
  `distribution_pattern`, `quantity_in_commerce`, `country`, `state`

---

## 6. Context Engine (Cross-Dataset Search + 360° View)

### 6.1 Search Logic
- Normalizes query to lowercase
- Uses `rapidfuzz` partial ratio across selected columns
- Returns results per dataset with a match score and record payload

### 6.2 Linkage
- If query matches a `k_number`, attempts to follow `predicate_k_numbers` and retrieve related 510(k) records.
- In 360° view, uses `product_code` from top 510(k) match to pull related:
  - recalls
  - MDRs
  - GUDID rows

### 6.3 KPI Derivation
- MDR count = number of matched ADR records for product code
- top recall class = worst severity among matched recalls (I > II > III)

---

## 7. Document Processing Workflows

### 7.1 PDF Upload and Preview
- PDF stored in-memory in session.
- Preview rendered via inline iframe using base64 PDF data URI.

### 7.2 Trimming
- User provides page ranges (e.g., `1-5,10`).
- Trimming implemented using PyPDF2 by copying selected pages into a new PDF byte stream.

### 7.3 Text Extraction
- PyPDF2 extracts text per page; joined into `raw_text`.
- `ocr_text` initially mirrors extracted text for convenience.

### 7.4 OCR Options
1. **Local OCR (Tesseract)**  
   - Convert pages to images using `pdf2image`
   - OCR each image using `pytesseract`

2. **Vision OCR (OpenAI/Gemini)**  
   - Convert pages to images using `pdf2image`
   - Send images to a Vision-capable model for improved extraction
   - Produces page-marked output for traceability

---

## 8. Agent Orchestration and Configuration

### 8.1 Configuration Inputs
- `agents.yaml` loaded and editable in sidebar.
- Standardization/validation performed with Pydantic:
  - Ensures presence of required keys
  - Fills missing provider/model/temperature/max_tokens defaults
- `SKILL.md` loaded and editable; appended to every agent’s system prompt at runtime.

### 8.2 Execution Controls
Before each run, user can:
- Select provider and model (OpenAI/Gemini/Anthropic/xAI)
- Set max tokens (UI defaults to **12000**)
- Edit system prompt and user prompt

### 8.3 Chaining
- Agent outputs stored in `agent_outputs[]`.
- Each run provides an editable `edited_output`.
- Next run can use:
  - last edited output (preferred)
  - OCR text
  - raw extracted text

This enables iterative refinement and human override between agent steps.

---

## 9. AI Note Keeper

### 9.1 Inputs
- Paste text/Markdown
- Upload PDF/TXT/MD (PDF uses PyPDF2 extraction)

### 9.2 Outputs
- Organized Markdown (editable)
- Coral-highlighted render view
- User-defined keyword-color highlights layered on top

### 9.3 AI “Magics”
- Organize Note
- Executive Summary
- Action Items + Owners
- Risk/Deficiency Finder
- Compliance Checklist Generator
- AI Keywords Highlighter (manual keyword-color pairs)

---

## 10. Dataset Studio (NEW in v2.1)

Dataset Studio extends the platform from “document-centric” to “dataset-centric” analysis.

### 10.1 Capabilities
For each dataset type (`510k`, `recall`, `adr`, `gudid`), the user can:

1. **Paste dataset** as CSV/JSON/text
2. **Upload dataset** file in CSV/JSON/TXT
3. **Parse & Load** into a DataFrame
4. **Standardize** into the canonical schema
5. **Preview** first 20 records (post-standardization)
6. **Download** standardized dataset as CSV/JSON
7. **Generate Summary** (1000–2000 words, Markdown, model selectable, prompt editable)
8. **Keyword Filter** to subset records
9. **Run Dataset Query** using LLM prompt over either full dataset context or filtered subset
10. **Edit results** in Markdown and view Coral render

### 10.2 Parsing Rules
- Format detection:
  - JSON if text begins with `{` or `[`
  - CSV heuristic if includes commas and newlines
  - Fallback attempts JSON then CSV
- JSON wrappers:
  - If JSON is an object, Dataset Studio tries to unwrap common keys:
    - `data`, `records`, `items`, `rows`
  - If still a single object, it is coerced into a list of one record.

### 10.3 Standardization Process
Standardization is done by mapping arbitrary columns to canonical fields using:
- Exact normalized matching of column names (alphanumeric only)
- Synonym lists per field (e.g., `k#`, `submission_number`, `mdr_id`)
- Fuzzy matching fallback (high threshold)

Then:
- Missing canonical fields are filled with `None`.
- Type normalization by dataset type:
  - 510(k) `predicate_k_numbers` coerced into a list (split strings by `,` or `;`).
  - GUDID booleans (`sterile`, `single_use`, `implantable`, `contains_nrl`) coerced from common truthy/falsey strings.
  - Recall `quantity_in_commerce` coerced to integer when possible.
- Rows that contain no meaningful signals across canonical fields are dropped.

A **Standardization Report** is generated as Markdown:
- Canonical field → source column mapping table
- Row count and original column count

### 10.4 Preview and Download
After standardization:
- Preview is limited to first 20 rows for usability.
- Downloads:
  - CSV via `df.to_csv(index=False)`
  - JSON via `orient="records"` pretty-printed

### 10.5 Dataset Summary (1000–2000 words)
Users can:
- Select provider/model and max tokens
- Edit a summary prompt
- Generate a comprehensive Markdown summary based on an LLM context package containing:
  - row count, column list
  - missingness table (top 20 missing)
  - top values for key columns (dataset-type dependent)
  - sample records as JSON (first N rows, default 50)

Summary is stored in session, editable, and rendered with Coral highlighting.

### 10.6 Dataset Query (Keyword Filter + LLM Prompt)
- Keyword filter performs row-wise substring matching across all fields, including list fields.
- User can:
  - preview filtered results (first 20 rows)
  - choose whether to use filtered set as LLM context
- User can run a model-selected LLM prompt over a context package built from either:
  - filtered rows, or
  - full dataset

Query output is editable in Markdown and rendered with Coral highlight.

### 10.7 Integration with Global Search
Dataset Studio updates the session DataFrames. Once loaded:
- The top bar dataset counts update.
- The global search and 360° device view use the updated DataFrames through the rebuilt search engine.

---

## 11. Security and Privacy

### 11.1 API Keys
Supported keys:
- `OPENAI_API_KEY`, `GEMINI_API_KEY`, `ANTHROPIC_API_KEY`, `XAI_API_KEY`

Rules:
- If present in environment variables: show “Managed by System”; never display the key.
- If absent: allow password input; store only in `st.session_state["api_keys"]`.
- Keys are never written to disk and should not be logged.

### 11.2 Data Handling
- PDFs processed in memory.
- Dataset uploads processed in memory.
- Only the necessary text/images are transmitted to LLM providers.
- “Danger Zone” allows clearing session state, removing in-session keys and artifacts.

---

## 12. Operational Considerations

### 12.1 Performance
- Fuzzy search is efficient for small to medium datasets.
- OCR and Vision OCR are CPU/time expensive; page trimming is encouraged.
- LLM dataset context includes sampling (default 50 rows) to control token usage.

### 12.2 Reliability and Error Handling
Common failure scenarios:
- Invalid dataset format → parse exceptions shown to user
- Invalid YAML → validation errors shown to user
- Missing API keys for selected provider → UI error
- Provider rate limits → surfaced as runtime exceptions

### 12.3 Streamlit State Safety (Bugfix Requirement)
To avoid `StreamlitAPIException`, the app ensures:
- Widget keys (e.g., `ds_input_text_widget`) do not match the session variable name (e.g., `ds_input_text`) that the code sets.

This is a critical requirement for stable operation.

---

## 13. Testing and Acceptance Criteria

### 13.1 Functional Acceptance Tests
1. UI switches theme, language, and painter style without breaking state.
2. Default datasets load and show correct counts.
3. Global search returns results across datasets; 360° view renders for known queries.
4. PDF trim + extract works; preview shows PDF; OCR populates OCR editor.
5. Agents:
   - load from agents.yaml
   - execute with selectable provider/model/tokens
   - outputs appear and can be edited
   - chaining uses edited output when selected
6. Note Keeper transforms notes, supports keyword coloring and rendered view.
7. Dataset Studio:
   - accepts pasted CSV/JSON and uploaded CSV/JSON/TXT
   - standardizes to canonical schema
   - previews first 20 rows
   - allows CSV/JSON download
   - generates dataset summary (1000–2000 words target) with selectable model
   - keyword filters rows and runs LLM query using filtered or full context
8. Clearing session resets all ephemeral secrets and artifacts.

---

## 14. Limitations and Roadmap

### 14.1 Known Limitations
- Large datasets: performance and token limits will constrain LLM context. Current design uses sampling rather than scalable retrieval.
- Keyword filtering: simple substring search; no boolean query language or field-specific filters yet.
- Evidence linking: outputs do not currently link back to exact PDF pages.

### 14.2 Recommended Enhancements
- Add advanced query language (field filters, AND/OR, regex).
- Add scalable RAG with embeddings for dataset Q&A and document Q&A.
- Add PII redaction layer before external API calls.
- Add charting for dataset profiling (missingness heatmap, distributions).
- Add click-to-jump references from agent output to PDF viewer and dataset row IDs.

---

# 20 Comprehensive Follow-Up Questions

1. Should Dataset Studio support **batch upload** of all four dataset types at once (auto-detect type per file) and generate a combined cross-dataset summary?
2. Do you want **strict schema enforcement** (block load if required fields missing) or keep the current tolerant standardization (fill with `None`)?
3. Should the standardization mapping UI allow **manual override** (user selects the source column for each canonical field) before finalizing?
4. Should standardization generate a **confidence score** per mapped field (exact vs synonym vs fuzzy) and display the top candidates?
5. For 510(k) predicate fields, should we support **multi-column predicate extraction** (predicate_1…predicate_n) and unify into `predicate_k_numbers`?
6. Should Recall `recall_class` be normalized aggressively (e.g., `Class 1` → `I`) and also validate allowed values?
7. Should Dataset Studio include optional **de-duplication** (by key ID fields) with a report of duplicates removed and examples?
8. Should Dataset Studio include **profiling metrics** (unique ratio, suspicious values, date parsing failures) and a “data quality score”?
9. Do you want Dataset Studio downloads to include **Excel (.xlsx)** and/or **Parquet** output for reviewer workflows?
10. Should dataset keyword filtering support **advanced expressions** (AND/OR, quoted phrase, field-specific query like `product_code:GAG`)?
11. Should filtered dataset rows be **pinnable** into Final Report as evidence tables (with user-selected columns)?
12. Should dataset summaries include a dedicated section for **regulatory signals** (e.g., Class I recall themes, MDR outcome distribution) tuned per dataset type?
13. Should Dataset Query become a **multi-turn chat** with stateful conversation memory rather than single-shot prompts?
14. Do you want a “**token budget estimator**” that previews how many tokens the dataset context will consume before calling the LLM?
15. Should the app support **cross-dataset joins** (e.g., product_code overlap between ADR and Recall) directly inside Dataset Studio?
16. For Vision OCR, should we add a tool to extract identifiers (K#, UDI, product_code) and auto-populate Global Search and Dataset Studio filters?
17. Should there be an “**agent input selector**” to feed Dataset Studio results (summary or query) directly into the agent chain as inputs?
18. Do you want a formal “**PII/PHI redaction mode**” before any dataset/document is sent to external APIs, with a redaction preview?
19. Should the global dashboard include dataset-studio KPIs (e.g., **data quality**, missingness %, duplicate count) alongside current device KPIs?
20. Do you want persistent storage (e.g., saving standardized datasets and reports) across sessions, and if so, should it be via **HF persistent storage**, external storage, or downloadable bundles only?
